{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "# Load and preprocess data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/heart.csv')\n",
        "\n",
        "# Split the features and the target variable\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values\n",
        "\n",
        "# Define the number of clients\n",
        "num_clients = 10\n",
        "\n",
        "# Calculate the size of each split\n",
        "split_size = len(df) // num_clients\n",
        "df = df[:(len(df)-100)]\n",
        "# Create a dictionary to store client data\n",
        "client_data = {}\n",
        "\n",
        "# Split data for 10 clients\n",
        "for i in range(num_clients):\n",
        "    # Define the start and end index for the split\n",
        "    start_idx = i * split_size\n",
        "    if i == num_clients - 1:  # For the last client, take all remaining data to handle any rounding issues\n",
        "        end_idx = len(df)\n",
        "    else:\n",
        "        end_idx = start_idx + split_size\n",
        "\n",
        "    # Split the data accordingly\n",
        "    X_train, y_train = X[start_idx:end_idx], y[start_idx:end_idx]\n",
        "\n",
        "    # Assign to client data dictionary\n",
        "    client_data[i] = {'train': (X_train, y_train)}\n",
        "\n",
        "# Now client_data[i] holds the training data for the i-th client\n",
        "# Define the model architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(X.shape[1], 50)\n",
        "        self.fc2 = nn.Linear(50, 20)\n",
        "        self.fc3 = nn.Linear(20, 2)  # Assuming binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.log_softmax(self.fc3(x), dim=1)\n",
        "\n",
        "# Training function for one epoch\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    for data, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Federated Averaging\n",
        "def federated_averaging(models):\n",
        "    global_model = SimpleNN()\n",
        "    global_state_dict = global_model.state_dict()\n",
        "\n",
        "    for key in global_state_dict.keys():\n",
        "        global_state_dict[key] = torch.mean(torch.stack([models[i].state_dict()[key] for i in range(len(models))]), 0)\n",
        "\n",
        "    global_model.load_state_dict(global_state_dict)\n",
        "    return global_model\n",
        "\n",
        "# Test the global model\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    targets_list = []\n",
        "    outputs_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            targets_list.extend(targets.numpy())\n",
        "            outputs_list.extend(outputs[:, 1].exp().numpy())  # Assuming the second column is probability of class 1\n",
        "\n",
        "    acc = correct / total\n",
        "    auc = roc_auc_score(targets_list, outputs_list)\n",
        "    return total_loss / len(test_loader), acc, auc\n",
        "\n",
        "# Main loop for federated training\n",
        "NUM_ROUNDS = 500\n",
        "clients_models = [SimpleNN() for _ in range(10)]\n",
        "\n",
        "for round in range(NUM_ROUNDS):\n",
        "    for i in range(10):\n",
        "        local_data = client_data[i]\n",
        "        train_dataset = TensorDataset(torch.FloatTensor(local_data['train'][0]), torch.LongTensor(local_data['train'][1]))\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "        optimizer = optim.Adam(clients_models[i].parameters(), lr=0.01)\n",
        "        criterion = nn.NLLLoss()\n",
        "        train_one_epoch(clients_models[i], train_loader, optimizer, criterion)\n",
        "\n",
        "    # Perform federated averaging\n",
        "    global_model = federated_averaging(clients_models)\n",
        "\n",
        "    # Update local models\n",
        "    for model in clients_models:\n",
        "        model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "# Evaluate the global model\n",
        "X_test, y_test = X[-100:], y[-100:]\n",
        "\n",
        "# Assign to client data dictionary\n",
        "client_data_test = {'test': (X_test, y_test)}\n",
        "test_data = np.concatenate([client_data_test['test'][0] for i in range(1)], axis=0)\n",
        "test_labels =np.concatenate([client_data_test['test'][1] for i in range(1)], axis=0)\n",
        "test_dataset = TensorDataset(torch.FloatTensor(test_data), torch.LongTensor(test_labels))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "loss, accuracy, auc_score = test_model(global_model, test_loader, criterion)\n",
        "print(f'Final AUC Score: {auc_score}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H_yqbSIMtdd",
        "outputId": "c9d02ffa-262c-4904-ddab-db285dae06b7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final AUC Score: 0.8850644122383253\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}